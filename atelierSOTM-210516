petit compte-rendu de l'atelier

participants
Noémie Lehuby, Pascal Rhod, Patrick Gendre, Yves Bonin, Marin, Florian Lainez, Ben Fournier, Denis Heifer, Thomas Gratier, Quentin, Frédéric Rodrigo, Silvio Rousic, Robin Vettier, JP Olivero?, Bertrand Billoud

Suite à la présentation du Cerema d'hier http://mim.cete-aix.fr/spip.php?article418, l'objectif est de voir comment avancer pour créer un projet BATO.
L'idée de reprendre le principe de BANO fait consensus : une base regroupant toutes les sources de données open data et OSM, des scripts de test/correction/fusion, une base fusionnée en conservant systématiquement la donnée OSM, où chaque arrêt avec ses propres identifiant BATO, mais en conservant les identifiants de chaque producteur de donnée.

Egalement, l'idée de se limiter aux arrêts physiques, sur le terrain, fait consensus.

Comme pour l'adresse dans la BANO, on sait qu'il y a plusieurs définitions d'un arrêt ; on peut considérer arbitrairement que pour la BATO les arrêts correspondent aux zones d'embarquement, mais pour commencer ce n'est pas essentiel. On sait aussi que des arrêts complexe comme la gare Montparnasse pourront être décrites beaucoup finement mais on peut commencer par un point.

Le projet BATO en tant que tel se concentre sur la donnée, c'est un autre sujet de savoir pour quelles applications les données seront utilisées. Dans quelques temps une fois qu'il y aura de la donnée suffisamment intéressante, les applications suivront.
Le projet met à dispo les données elles-mêmes, mais aussi les résultats des scripts, source de données par source de données, et globalement, et éventuellement des outils complémentaires.
Aux producteurs de données open data ensuite d'aller voir les résultats et d'en tirer parti pour améliorer leur qualité.
Concernant les données OSM, comme pour la BANO, l'idée n'est PAS d'importer en masse les données open data, mais d'apporter à la communauté des infos utiles pour compléter progressivement les arrêts dans OSM, avec une validation sur le terrain.

Les prochaines étapes sont:
- créer un dépôt github et une organisation BATO est pris, on prend BATO-FR
- le CEREMA fournit (sur le dépôt ou à une autre adresse) son état actuel d'avancement ; le Cerema essaiera de contribuer sur la documentation notamment et dans les discussions, mais ne pourra sans doute guère contribuer techniquement à la mise au point des scripts ;
    - liste des sources de données (opendata GTFS/Neptune et fichiers CSV/SHP)
    - export de la base actuelle présentée hier (dump postgres et/ou fichiers CSV)
    - scripts
- écrire les scripts d'automatisation d'import
- faire tourner les scripts (en utilisant les serveur OSM-FR)
- mettre à dispo les résultats (données, fichiers de résultats des tests, visualisation carto) : première version agrégée des données existantes (en conservant tous les attributs des données sources), à partir de là il devrait y avoir des volontaires pour aller plus loin!
En termes de scripts de contrôle de la donnée, il faut bien sûr s'appuyer sur les outils existants à commencer par Osmose.

Dans un 2ème temps seulement, il faudra travailler à la déduplication, il n'est pas utile de savoir définir dès aujourd'hui comment on va former l'identifiant BATO (ID proposé Netex, geohash, ID purement technique...?)
On peut au départ avoir une règle ultra simple (on fusionne uniquement des arrêts de mêmes coordonnées et même nom) et on raffinera progressivement. Certains cas nécessiteront forcément une validation manuelle.
Il faudra aussi définir quels attributs on garde dans la BATO par rapport à ceux des données source, et lesquels on ajoute, aussi.
Ce sera ensuite certainement utile de fournir une API d'accès aux données (géocodage etc.), mais là encore ce sera dans un 2ème temps.

Merci à tous pour cet atelier super constructif, bienvenue à tous pour faire avancer le projet!
